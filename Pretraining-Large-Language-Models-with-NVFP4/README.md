
# Pretraining Large Language Models with NVFP4

## 革新

这篇论文的最大突破是**首次实现了 4 位浮点数（FP4）训练超大规模语言模型（12B 参数，10T tokens）**，达到了与 FP8 训练相当的精度，为未来 LLM 训练效率提升打开了新的可能性。

## 论文解析

### 为什么需要更低精度的训练？

想象你在建造一座摩天大楼，如果能用更轻的材料达到相同的强度，你就能：
- 节省一半的材料成本
- 建造速度提升 2-3 倍
- 能源消耗大幅降低

大语言模型训练也是如此：
- 训练一个前沿模型需要 **数十到数百 yottaflops** 的计算量
- FP8 训练已经被广泛采用
- **FP4 训练可以带来**：
  - 相比 FP8 **算力提升 2-3 倍**
  - **内存使用减半**
  - **能耗显著降低**

### 现有方法的问题

在 FP4 精度下训练大模型面临三大挑战：

1. **数值表示范围极窄**
   - FP4 只有 1 位符号、2 位指数、1 位尾数
   - 可表示的值极其有限：±0, ±0.5, ±1, ±1.5, ±2, ±3, ±4, ±6
   - 动态范围不足导致数值溢出或归零

2. **训练不稳定**
   - 量化误差在长训练过程中累积
   - 梯度更新容易出现偏差
   - 大规模模型（数十亿参数）和长 token 序列（数万亿）放大了这些问题

3. **实现复杂度高**
   - 需要在前向和反向传播中保持数值一致性
   - 异常值（outliers）破坏量化效果
   - 不同层对精度敏感度不同

核心矛盾是：**如何在极窄的 4 位精度下，保证数万亿 tokens 的训练稳定性和模型质量？**

### NVFP4 的解决方案

论文提出了 NVFP4 格式和配套的训练方法论，包含四大核心创新：

#### 1. NVFP4 数据格式：精细化微缩放

NVFP4 改进了传统的 MXFP4（microscaling FP4）格式：

**传统 MXFP4 的局限**：
- 32 个元素共享一个 8 位缩放因子
- 缩放因子格式为 UE8M0（无符号、8 位指数、0 位尾数）
- 只能表示 2 的整数次幂，精度有限

**NVFP4 的改进**：
```
NVFP4 = 局部微缩放 + 全局张量缩放

1. 微块（Micro-block）：
   - 更小的块大小（16 个元素）
   - FP8 格式的缩放因子（E5M2，带小数精度）
   - 更精确地捕捉局部动态范围

2. 全局缩放：
   - FP32 张量级缩放因子
   - 确保整体数值稳定
```

![NVFP4 矩阵存储格式](./assets/images/A%2016×32%20matrix%20stored%20in%20NVFP4%20format.png)

**为什么这么做？**

- **更小的块**：16 vs 32，能更精细地适应局部数值分布
- **FP8 缩放因子**：相比 UE8M0 的 2^n 精度，FP8 可以表示如 1.5×2^n 这样的值
- **两级缩放**：micro-block 处理局部，tensor-level 处理全局，双重保险

![NVFP4 vs MXFP4 对比](./assets/images/NVFP4%20vs%20MXFP4%20comparisons.png)

#### 2. 随机 Hadamard 变换（RHT）：驯服异常值

神经网络权重和激活中常有异常大的值（outliers），它们会：
- 强制放大整个块的缩放因子
- 导致其他正常值精度损失

**Hadamard 变换的作用**：
```python
# 将权重梯度乘以随机 Hadamard 矩阵
W_grad_transformed = HadamardMatrix @ W_grad @ HadamardMatrix.T

# 效果：
# - 将异常值的能量分散到多个元素
# - 使数值分布更接近高斯分布
# - 降低块内的动态范围
```

关键设计选择：
- **只对 Wgrad（权重梯度）应用**：实验发现对 Fprop 和 Dgrad 应用反而降低精度
- **16×16 矩阵大小**：平衡了效果和计算开销
- **固定随机种子**：训练全程使用同一个随机向量，避免过度随机性

![计算流程示意](./assets/images/Illustration%20of%20compute%20flow%20for%20a%20NVFP4%20quantized%20linear%20layer.png)

#### 3. 二维（2D）块缩放：前向后向一致性

传统量化在前向和反向传播中可能产生不一致的量化表示：

```
问题案例：
前向传播: W --沿输入维度缩放--> Q(W)
反向传播: W^T --沿输出维度缩放--> Q(W^T)
          ↓
     两个不同的量化版本！
```

**NVFP4 的 2D 缩放方案**：
- 使用 16×16 的块缩放因子
- 同时考虑行和列的缩放
- 确保 W 和 W^T 的量化表示数值一致

**为什么重要？**
- 权重的不一致量化会影响激活梯度
- 误差通过反向传播在层间累积
- 在数万亿 tokens 的长训练中，一致性至关重要

#### 4. 随机舍入（Stochastic Rounding）：无偏梯度估计

传统的"四舍五入"会引入系统性偏差：

```
例子：
真实梯度: [0.3, 0.3, 0.3, 0.3, 0.3]  (总和 = 1.5)
四舍五入: [0,   0,   0,   0,   0]    (总和 = 0)
          ↓
      梯度消失！
```

**随机舍入的做法**：
```python
# 根据小数部分的大小，概率性地向上或向下取整
if random() < (value - floor(value)):
    quantized = ceil(value)
else:
    quantized = floor(value)
```

关键发现：
- **只对梯度应用**（Wgrad 和 Dgrad 的梯度输入）
- **不对激活和权重应用**：会引入过多噪声导致发散
- **保证梯度更新的期望无偏**：E[量化梯度] = 真实梯度

### 技术细节

#### 混合精度策略

不是所有层都能承受 FP4 的精度损失：

**保持 FP8/BF16 精度的层**：
1. **嵌入层（Embedding）**：输入输出敏感
2. **最终的线性层**：直接影响输出质量
3. **最后几个 Transformer 块**：决定最终表示质量
4. **LayerNorm 和激活函数**：数值稳定性关键

**对 12B 模型的具体配置**：
- 总共 40 层，**前 36 层线性层用 NVFP4**
- 最后 4 层保持 FP8
- 嵌入和输出层用 BF16

![消融实验结果](./assets/images/Ablations%20on%20the%2012B%20model%20trained%20for%2010T%20tokens.png)

#### 动态精度切换

训练过程中可以调整精度策略：

```
训练阶段划分：
0-1T tokens:   部分 FP4（预热）
1T-10T tokens: 全面 NVFP4
```

如果训练不稳定，可以：
- 临时切换回 FP8
- 稳定后再切回 NVFP4
- 最后阶段可以全程 FP8 确保质量

#### 量化流程

完整的 NVFP4 量化包含以下步骤：

1. **全局缩放**：计算张量级 FP32 缩放因子
2. **分块**：将张量划分为 16 元素的微块
3. **局部缩放**：为每个微块计算 FP8 缩放因子
4. **量化**：将缩放后的值映射到 FP4 可表示范围
5. **存储**：紧凑存储 FP4 值和两级缩放因子

## 实际效果有多震撼？

### 史无前例的训练规模

这是迄今为止公开报道的最大规模 FP4 训练：

- **模型规模**：12B 参数（hybrid Mamba-Transformer）
- **训练数据**：10T tokens
- **训练时长**：数千 GPU-小时
- **成果**：loss 曲线和下游任务精度与 FP8 基线相当

![训练损失曲线](./assets/images/Validation%20loss%20of%20NVFP4%20and%20FP8%20pretraining%20for%20the%2012B%20model%20using%2010T%20tokens.png)

### 下游任务性能

在多个标准评测基准上的表现：

| 任务 | FP8 基线 | NVFP4 训练 | 差异 |
|------|----------|------------|------|
| **MMLU-pro** | 62.62% | **62.58%** | -0.04% |
| **GSM8K** | ~85% | ~84.5% | -0.5% |
| **HumanEval** | ~52% | ~51% | -1% |

**关键发现**：
- NVFP4 训练的模型在 MMLU-pro 上几乎完全匹配 FP8 基线
- 其他任务的性能差异在可接受范围内（< 1%）
- 证明了 FP4 训练的可行性

![任务准确率对比](./assets/images/Task%20accuracy%20of%20NVFP4%20versus%20FP8%20measured%20throughout%2010T%20tokens%20of%20pretraining.png)

### 消融实验的重要发现

论文进行了详尽的消融实验，验证每个组件的重要性：

**1. Hadamard 变换的影响**：
- 不使用 RHT：loss 上升 ~3%
- 只在 Wgrad 使用 RHT：最佳效果
- 在 Fprop/Dgrad 使用 RHT：反而降低精度

**2. 随机舍入的影响**：
- 不使用随机舍入：训练发散
- 只对梯度应用：稳定收敛
- 对激活/权重应用：训练发散

**3. 2D 缩放的影响**：
- 1D 缩放（不一致）：loss 增加 ~2%
- 2D 缩放（一致）：最佳效果

**4. 混合精度的影响**：
- 全 FP4：训练不稳定
- 最后 4 层 FP8：稳定且精度最优

### 与 MXFP4 的对比

论文也对比了使用 MXFP4 格式训练的效果：

| 格式 | 块大小 | 缩放因子格式 | 训练稳定性 | 最终 loss |
|------|--------|--------------|-----------|-----------|
| MXFP4 | 32 | UE8M0 (整数幂) | 较差 | 更高 |
| **NVFP4** | 16 | FP8 (带小数) | 优秀 | **更低** |

NVFP4 的优势：
- 更精细的局部量化（16 vs 32）
- 更高精度的缩放因子（FP8 vs UE8M0）
- 更好的训练稳定性

## 技术架构总结

NVFP4 训练方法论采用了**分层优化**设计：

```
数据格式层:   [NVFP4格式]
              - 16元素微块
              - FP8局部缩放 + FP32全局缩放
                ↓
数值处理层:   [Hadamard变换] → [2D块缩放]
              - 分散异常值
              - 保证前向后向一致性
                ↓
量化策略层:   [随机舍入] + [混合精度]
              - 梯度无偏估计
              - 敏感层高精度
                ↓
训练流程:     [稳定的FP4训练]
```

核心设计哲学：
1. **格式优化**：更精细的表示能力
2. **数值鲁棒**：驯服异常值，保证一致性
3. **统计无偏**：随机舍入保证梯度期望正确
4. **选择性精度**：在关键位置保持高精度

这种设计就像给训练过程装上"减震器"：通过多层次的优化，在保证数值稳定的前提下，最大化利用低精度带来的效率提升。

## 适用场景

NVFP4 训练特别适合以下场景：

1. **超大规模预训练**：数十亿参数、数万亿 tokens 的模型
2. **资源受限环境**：GPU 内存或算力有限
3. **能效敏感应用**：降低训练能耗和成本
4. **下一代硬件**：NVIDIA Blackwell 等原生支持 FP4 的加速器
5. **研究探索**：探索极低精度训练的可能性边界

## 局限性与未来方向

当前研究的局限：

1. **硬件支持**：需要 NVIDIA Blackwell 等支持 FP4 的硬件才能获得实际加速
2. **实现复杂度**：需要深度定制训练框架（如 Transformer Engine）
3. **通用性待验证**：主要在 Mamba-Transformer 混合模型上验证，其他架构需要进一步测试
4. **超参数敏感**：Hadamard 矩阵大小、混合精度配置需要针对不同模型调优

未来可能的改进方向：

- **自适应精度**：根据训练阶段和层敏感度自动调整精度
- **更小精度**：探索 3-bit 甚至 2-bit 训练
- **端到端优化**：从模型架构到硬件的协同设计
- **推理应用**：将训练技术迁移到推理量化

## 实现建议

根据论文，想要复现 NVFP4 训练的关键配置：

### 数据格式
- **微块大小**：16 个元素
- **局部缩放**：FP8 (E5M2) 格式
- **全局缩放**：FP32 张量级缩放

### Hadamard 变换
- **矩阵大小**：16×16
- **应用位置**：仅 Wgrad 的输入
- **随机种子**：训练全程使用单一固定种子

### 舍入策略
- **随机舍入**：应用于 Wgrad 和 Dgrad 的梯度输入
- **最近舍入**：应用于激活和权重

### 混合精度
- **Transformer 块**：前 90% 用 NVFP4，后 10% 用 FP8
- **特殊层**：嵌入、输出线性层、LayerNorm 保持 FP8/BF16
- **优化器状态**：始终保持 FP32

### 训练监控
- 密切监控 loss 曲线的稳定性
- 如果出现 spike，考虑临时切换到 FP8
- 可以在训练后期全程使用 FP8 确保收敛

## 相关资源

- [NVFP4 论文原文](https://arxiv.org/html/2509.25149v1)
- [Transformer Engine (NVFP4 支持)](https://github.com/NVIDIA/TransformerEngine)
- [NVIDIA Blackwell 架构](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)
- [Kcores LLM Arena](https://llm-arena.kcores.com)

## 总结

NVFP4 论文标志着 LLM 训练精度降低的一个重要里程碑。通过精心设计的数据格式、数值处理技术和训练策略，首次实现了在 4 位浮点精度下训练超大规模语言模型。虽然当前主要是算法层面的突破，实际加速还需要硬件支持，但它为未来更高效、更节能的 AI 训练指明了方向。

随着 NVIDIA Blackwell 等新一代硬件的普及，NVFP4 训练有望在未来 1-2 年内成为大模型训练的标准配置，为 AI 的民主化和可持续发展做出重要贡献。


