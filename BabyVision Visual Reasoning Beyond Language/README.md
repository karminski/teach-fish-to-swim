
# BabyVision: Visual Reasoning Beyond Language

## 革新

BabyVision 这篇论文揭示了一个令人震惊的事实：**当今最先进的多模态大模型（MLLMs）在人类幼儿都能轻松解决的基础视觉任务上惨遭失败**。论文提出的 BABYVISION 基准测试涵盖 388 道题目，横跨 4 大类别、22 个子任务，系统性地评估了 MLLMs 的"婴儿级"视觉能力。实验结果令人深思：**最强模型 Gemini3-Pro-Preview 仅获得 49.7 分，而成年人平均得分高达 94.1 分**，甚至落后于 6 岁儿童的 65 分。这表明当前 MLLMs 在知识密集型任务上的优异表现，实际上是在**用语言先验掩盖其脆弱的视觉理解能力**。

## 论文解析

### 核心发现：倒置的能力画像

人类的视觉理解发展路径是：**先有视觉，后有语言**。婴儿在出生后几个月内就能辨别形状、纹理、追踪运动物体、推断遮挡和深度关系。这些"早期视觉能力"为后续的语言、抽象推理和运动规划奠定基础。

然而，当今的 MLLMs 呈现出**完全相反的能力画像**：

| 能力类型 | 人类发展顺序 | MLLMs 表现 |
|---------|------------|-----------|
| 基础视觉（形状、追踪、空间） | **最先发展** | ❌ 极弱 |
| 语言理解 | 中期发展 | ✅ 很强 |
| 知识推理（数学、专业知识） | **最晚发展** | ✅ 很强 |

**关键洞察**：MLLMs 在需要大量教育和专业知识的高层任务上表现出色（如 HLE、MMMU、MathVista），却在 3 岁儿童都能解决的基础视觉任务上频频翻车。这说明它们**并非真正"看见"了图像，而是在用语言知识"猜测"答案**。

![人类与 MLLMs 性能对比](./assets/images/Comparison%20of%20Human%20vs%20MLLMs%20Performance.jpg)

### BABYVISION 基准测试设计

BABYVISION 的设计哲学是：**最小化对语言知识的依赖，最大化对纯视觉感知的考察**。

#### 四大类别

| 类别 | 核心能力 | 典型任务 | 人类基准 |
|-----|---------|---------|---------|
| **Fine-grained Discrimination**<br>精细辨别 | 观察非语言细节 | 找相同/不同、数图案、补全形状 | 92.3% |
| **Visual Tracking**<br>视觉追踪 | 流形理解 | 连线追踪、迷宫、地铁路线图 | 94.6% |
| **Spatial Perception**<br>空间感知 | 空间想象 | 3D方块展开、视角转换、数方块 | 94.7% |
| **Visual Pattern Recognition**<br>视觉模式识别 | 模式归纳 | 逻辑图案、旋转/镜像/叠加规律 | 97.8% |

#### 数据规模

- **388 道独特题目**
- **22 个子类别**
- **4 大核心领域**
- 每道题目都经过精心设计，确保无法通过语言推理或知识检索"作弊"

### 四大挑战：MLLMs 为何失败？

#### 挑战 1：观察非语言细节（Observing Non-Verbal Details）

**问题**：MLLMs 难以从复杂图像中精确识别和比较细微的视觉差异。

![挑战1：观察非语言细节](./assets/images/Challenge%201%20Observing%20Non-Verbal%20Details.png)

**典型任务**：给定一个蜂窝图案，找出其中缺失的部分。

**失败原因**：
- 无法精确计数密集排列的元素
- 难以识别复杂图案中的空白区域
- 缺乏对几何形状精确匹配的能力

**人类表现**：92.3% | **最强模型**：46.2%（Gemini3-Pro-Preview）

---

#### 挑战 2：流形理解（Manifold Understanding）

**问题**：MLLMs 在追踪交织的线条时会"丢失身份"——当多条曲线相交时，模型无法正确判断哪条线通向哪里。

![挑战2：流形理解](./assets/images/Challenge%202%20Manifold%20Understanding.png)

**典型任务**：追踪一条从起点到终点的曲线，即使它与其他线条多次相交。

**失败原因**：
- **流形身份丢失**：在交叉点处错误切换到另一条线
- 缺乏对曲线连续性的理解
- 无法进行视觉上的"手指追踪"

**人类表现**：94.6% | **最强模型**：43.4%（Gemini3-Pro-Preview）

---

#### 挑战 3：空间想象（Spatial Imagination）

**问题**：MLLMs 无法在脑海中对 3D 结构进行心理变换——旋转、展开、从不同角度观察。

![挑战3：空间想象](./assets/images/Challenge%203%20Spatial%20Imagination.png)

**典型任务**：给定 3D 方块堆叠，判断从某个方向看会是什么形状。

**失败原因**：
- **空间想象失败**：无法心理旋转 3D 物体
- 缺乏对遮挡关系的理解
- 无法将 2D 展开图映射回 3D 结构

**人类表现**：94.7% | **最强模型**：53.7%（Gemini3-Pro-Preview）

---

#### 挑战 4：视觉模式归纳（Visual Pattern Induction）

**问题**：MLLMs 难以从视觉序列中归纳出潜在的变换规则，并预测下一个元素。

![挑战4：视觉模式归纳](./assets/images/Challenge%204%20Visual%20Pattern%20Induction.png)

**典型任务**：观察一系列图形的变化规律（旋转、颜色交换等），预测下一个应该是什么。

**失败原因**：
- 无法识别视觉元素的系统性变换
- 难以建立"变化规则"的抽象表示
- 缺乏对颜色、位置、方向的组合推理

**人类表现**：97.8% | **最强模型**：53.9%（Gemini3-Pro-Preview）

### 实验结果：令人震惊的差距

#### 闭源模型详细性能

![闭源MLLMs在BabyVision上的性能](./assets/images/Performance%20of%20Closed%20Source%20MLLMs%20on%20BabyVision.png)

| 模型 | 总分 | 精细辨别 | 视觉追踪 | 空间感知 | 模式识别 |
|-----|------|---------|---------|---------|---------|
| **Human** | **94.1** | 92.3 | 94.6 | 94.7 | 97.8 |
| Gemini3-Pro-Preview | 49.7 | 46.2 | 43.4 | 53.7 | 53.9 |
| GPT-5.2 | 34.4 | 27.3 | 34.9 | 35.2 | 54.9 |
| Doubao-1.8 | 30.2 | 39.2 | 15.7 | 24.7 | 37.7 |
| Qwen3-VL-Plus | 19.2 | 21.8 | 11.5 | 18.1 | 25.5 |
| Claude-4.5-Opus | 14.2 | 14.3 | 13.7 | 12.8 | 17.0 |
| Grok-4 | 16.2 | 11.0 | 24.1 | 13.2 | 24.8 |

**关键发现**：
- 最强模型 Gemini3-Pro-Preview（49.7%）仍比**成人低 44.4%**
- 所有模型在**视觉追踪**和**空间感知**上表现最差
- **Claude-4.5-Opus 和 Grok-4 甚至不如随机猜测**（25%）

#### 与不同年龄人类的对比

| 被试 | 平均得分 |
|-----|---------|
| 3 岁儿童 | ~40% |
| **Gemini3-Pro-Preview** | **~45%** |
| 6 岁儿童 | ~65% |
| 10 岁儿童 | ~74% |
| 12 岁儿童 | ~87% |
| 成年人 | ~94% |

**震撼结论**：**最强的 MLLM 勉强与 3 岁儿童打成平手，远远落后于 6 岁儿童。**

#### 各类别差距一致性

![各类别差距一致性](./assets/images/These%20gaps%20relative%20to%20humans%20are%20consistent%20across%20categories.png)

雷达图清晰展示：**人类（黑色虚线）在所有 22 个子任务上都接近满分**，而所有 MLLMs（彩色线）在所有维度上都大幅塌缩。这不是某一类任务的失败，而是**系统性的视觉能力缺陷**。

### BABYVISION-GEN：生成式视觉推理

论文还提出了 **BABYVISION-GEN**，一个评估视觉生成模型推理能力的配套基准。

#### 设计理念

人类解决视觉问题时，往往是**用视觉来解决视觉**——画图、追踪、标记。BABYVISION-GEN 要求模型：
- 不是输出文字答案
- 而是**生成正确的图像**来展示答案

#### 实验结果

| 模型 | 准确率 |
|-----|-------|
| Sora 2 | 3.3% |
| Veo 3 | 5.6% |
| BAGEL | 8.9% |
| Nano-Banana (SOTA) | 13.3% |

**结论**：即使是最先进的图像/视频生成模型，在视觉推理任务上也几乎完全失败。

### 自动评估工具

为了支持大规模评估，论文开发了自动评估工具：
- **与人类判断的一致率**：96%
- 支持文本输出和图像生成的双重评估
- 开源发布，便于社区复现

### GRPO 强化学习尝试

论文尝试使用 **GRPO（Group Relative Policy Optimization）** 对开源模型进行视觉推理强化学习：

| 指标 | 原始模型 | GRPO 后 | 提升 |
|-----|---------|---------|-----|
| 平均准确率 | 13.1% | 17.9% | +4.8% |

**发现**：
- 强化学习带来了一定提升（+4.8%）
- 但距离人类水平仍有巨大差距
- 表明**视觉能力的根本缺陷难以通过后训练弥补**

## 深层原因分析

### 为什么 MLLMs 在高级任务上成功，却在基础任务上失败？

1. **语言先验的掩盖作用**
   - 高级任务（数学、知识问答）可以通过语言推理"绕过"视觉
   - 基础视觉任务无法用语言知识替代

2. **训练数据的偏差**
   - 大量训练数据是"图文对"，模型学会了用文字描述图像
   - 很少有训练数据要求纯视觉推理

3. **评估方式的局限**
   - 现有基准测试过于依赖知识检索
   - BABYVISION 是首个系统性评估"超越语言"视觉能力的基准

### 与现有基准的区别

| 基准 | 核心考察 | 可被语言绕过？ |
|-----|---------|--------------|
| MMMU | 大学级专业知识 | ✅ 是 |
| MathVista | 数学推理 | 部分 |
| MME | 多模态理解 | ✅ 是 |
| BLINK | 视觉感知 | 部分 |
| **BABYVISION** | **纯视觉推理** | ❌ **否** |

## 对未来研究的启示

### 1. 视觉能力不应被忽视

当前 MLLM 的发展过于关注"更多知识"、"更强推理"，却忽略了**最基础的视觉感知能力**。

### 2. 需要新的训练范式

- 加入更多纯视觉推理任务
- 减少对语言先验的依赖
- 从婴儿视觉发展中汲取灵感

### 3. 评估体系需要完善

- BABYVISION 填补了一个重要空白
- 未来需要更多"无语言"视觉基准

### 4. 架构创新的必要性

当前的 Vision Transformer + LLM 架构可能存在根本局限：
- 图像被切分为 patch，丢失了全局结构
- 视觉信息被强制映射到语言空间

## 类比理解

**MLLMs 像一个博览群书但从未睁眼看世界的人**：
- 读过关于"苹果是红色的"，但从未真正看过苹果
- 知道"迷宫可以用右手法则解"，但无法真正追踪线条
- 学过"3D 物体可以旋转"，但无法在脑海中旋转

**BABYVISION 就是给它们做一次"视力检查"——结果是：近乎失明。**

## 相关链接

- [论文原文 (arXiv)](https://arxiv.org/pdf/2601.06521v1)
- [官方主页](https://unipat.ai/blog/BabyVision)
- [GitHub 代码](https://github.com/UniPat-AI/BabyVision)
- [Kcores LLM Arena](https://llm-arena.kcores.com)

## 论文信息

- **作者**：Liang Chen, Weichu Xie, Yiyan Liang, Hongfeng He, Hans Zhao et al.
- **机构**：UniPat AI, xbench, Alibaba, Moonshot AI, StepFun, Peking University, Tsinghua University 等
- **发表时间**：2026年1月13日
- **关键词**：Visual Reasoning, Multimodal LLMs, Benchmark, Core Vision Skills, Beyond Language


